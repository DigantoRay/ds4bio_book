{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Evolution of Large Language Models to Transformer Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "1. **Overview of Large Language Models (LLMs)**\n",
    "    - Definition and significance\n",
    "    - Historical perspective: from n-grams to neural networks\n",
    "2. **The Evolution to Transformer Models**\n",
    "    - Limitations of previous models (RNNs, LSTMs)\n",
    "    - Introduction of the Transformer architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Large language models (LLMs) excel at understanding and generating human language, powering many NLP tasks like translation, summarization, and text generation.\n",
    "N-grams: Used simple statistical methods to predict the next word based on preceding words.\n",
    "Neural Networks Evolved from basic perceptrons to advanced models like RNNs and LSTMs.\n",
    "\n",
    "RNNs and LSTMs, while good at handling sequences, struggle with vanishing gradients. Transformers overcome this by using an attention mechanism, which enables parallel processing and better handling of long-range dependencies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Section 1: Basics of Language Models\n",
    "1. **What is a Language Model?**\n",
    "    - Definition and purpose\n",
    "    - Applications in NLP\n",
    "2. **Mathematical Foundations**\n",
    "    - Probability theory basics\n",
    "    - N-gram models and Markov assumptions $$P(w_i), P(w_i|w_{i-1}), P(w_i|w_{i-1}, w_{i-2})$$ $$P(w_i|w_{i-1}, w_{i-2}, ..., w_{i-n})$$\n",
    "    - Example: Simple clinical note sentence modeling\n",
    "      - \"The patient is healthy.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "A language model assigns a probability to a sequence of words. This helps predict the next word in a sentence, generate text, and understand the context in language. Applications include speech recognition, machine translation, and sentiment analysis.\n",
    "\n",
    "Let’s explore the mathematical foundations. We calculate the probability of a sequence of words. This means the probability of the first word, followed by \n",
    "second word, up to the n-th word.\n",
    "In N-gram Models we have three types:\n",
    "\n",
    "Unigram: Assumes each word is independent, so we calculate the probability of each word separately.\n",
    "Bigram: Takes into account the previous word. It’s expressed as P of w sub i given w sub i minus one.\n",
    "Trigram: Extends this to two previous words, so it’s \n",
    "P of w sub i given w sub i minus one and w sub i minus two.\n",
    "The Markov Assumption simplifies our model by saying that the probability of a word depends only on a fixed number of preceding words, rather than the entire sequence.\n",
    "\n",
    "For example: To model the sentence 'The patient is healthy' using a bigram model, we calculate the probability as:\n",
    "\n",
    "'The probability of the sentence \"The patient is healthy\" equals the probability of the word \"The,\" multiplied by the probability of \"patient\" given \"The,\" multiplied by the probability of \"is\" given \"patient,\" and finally, multiplied by the probability of \"healthy\" given \"is.\"'\n",
    "\n",
    "This example shows how we build the overall probability of the sentence from the probabilities of individual word transitions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Section 2: Neural Networks and Language Models\n",
    "1. **Introduction to Neural Networks**\n",
    "    - Perceptrons and multi-layer networks\n",
    "    - Activation functions and their roles\n",
    "2. **Recurrent Neural Networks (RNNs)**\n",
    "    - Structure and working\n",
    "    - Backpropagation through time (BPTT)\n",
    "    - Limitations: Vanishing and exploding gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "\"Neural networks consist of layers of interconnected neurons. Each neuron transforms its input using a weight, bias, and activation function. Common activation functions are:\n",
    "\n",
    "Sigmoid: Output between 0 and 1.\n",
    "Tanh: Output between -1 and 1.\n",
    "ReLU: Efficient, outputs from 0 to positive values.\"\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are ideal for sequential data, maintaining a hidden state that remembers previous inputs. Training uses Backpropagation Through Time (BPTT), but RNNs can struggle with vanishing or exploding gradients, which make learning long-term dependencies challenging.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Section 3: The Transformer Model\n",
    "1. **Introduction to Transformers**\n",
    "    - Why Transformers? Overcoming RNN limitations\n",
    "    - Key concepts: Attention mechanism, self-attention\n",
    "2. **Detailed Architecture**\n",
    "    - Encoder and decoder structure\n",
    "    - Positional encoding\n",
    "    <span style=\"font-size: 0.8em; display: block; margin-left: 80px;\">$PE(pos, 2i) = sin(pos / 10000^{2i/d_{model}})$ <br>\n",
    "    $PE(pos, 2i+1) = cos(pos / 10000^{2i/d_{model}})$</span>\n",
    "    - Multi-head attention\n",
    "    <span style=\"font-size: 0.7em; display: block; margin-left: 80px;\">$MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O$\n",
    "    $head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$</span>\n",
    "    - Feedforward neural networks\n",
    "    <span style=\"font-size: 0.8em; display: block; margin-left: 80px;\">$FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$</span>\n",
    "    - Layer normalization and residual connections\n",
    "    <span style=\"font-size: 0.8em; display: block; margin-left: 80px;\">$LayerNorm(x + Sublayer(x))$</span>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Transformers revolutionize the handling of sequences by replacing recurrence with an attention mechanism, which allows for parallel processing of data. This overcomes the limitations of RNNs and LSTMs, enabling better learning of long-range dependencies. The key concepts include:\n",
    "Attention Mechanism: this step Computes a weighted sum of input values. The weights reflect the relevance of each value in the current context.\n",
    "And self-Attention: where Each word in the sequence evaluates its importance by considering all the other words in the sequence.\n",
    "\n",
    "\"Let’s look at the detailed architecture of transformers:\n",
    "\n",
    "first Encoder and Decoder Structure:\n",
    "\n",
    "Encoder Contains multiple layers. Each layer has a self-attention mechanism and a feedforward neural network.\n",
    "Decoder Also has multiple layers like the encoder but adds an extra attention mechanism to focus on the encoder’s output.\"\n",
    "\n",
    "Next Positional Encoding\n",
    "\"The position of each word is encoded to retain the order of the sequence. It’s defined as:\n",
    "P E at position pos and even index 2i equals the sine of the position divided by ten thousand raised to the power of two i over the model dimension. For odd indices, we use the cosine function instead.'\n",
    "\n",
    "Next Multi-Head Attention:\n",
    "\"Multi-head attention allows the model to focus on different parts of the input sequence from multiple perspectives simultaneously. This means, 'For multi-head attention, we concatenate the outputs of several attention heads and multiply by a weight matrix. Each head computes attention using queries, keys, and values transformed by their respective weight matrices.'\n",
    "\n",
    "Next component of the transformer is Feedforward Neural Networks:\n",
    "\"Each layer in the transformer also includes a feedforward neural network. The feedforward network applies a linear transformation to x using weight matrix W1 and bias b1, then a ReLU activation, followed by another linear transformation using weight matrix W2 and bias b2.\"\n",
    "\n",
    "Finally,\n",
    "Layer Normalization and Residual Connections:\n",
    "\"Transformers use layer normalization and residual connections to stabilize training. Layer normalization is applied to the sum of the input x and the output of the sublayer, which helps in managing the network’s depth and improving gradient flow.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Section 4: Mathematical Breakdown of Transformers\n",
    "1. **Self-Attention Mechanism**\n",
    "    - Mathematical formulation\n",
    "    <span style=\"font-size: 0.7em; display: block; margin-left: 80px;\"> $Q = XW^Q, K = XW^K, V = XW^V$</span>\n",
    "    <span style=\"font-size: 0.8em; display: block; margin-left: 80px;\">$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$ </span>\n",
    "    - Example: Attention calculation on clinical notes\n",
    "        - \"Patient has fever.\"\n",
    "2. **Positional Encoding**\n",
    "    - Need for positional information\n",
    "    - Mathematical representation and examples\n",
    "    <span style=\"font-size: 0.7em; display: block; margin-left: 80px;\"> $PE(pos, 2i) = sin(pos / 10000^{2i/d_{model}})$ </span>\n",
    "    <span style=\"font-size: 0.7em; display: block; margin-left: 80px;\">$PE(pos, 2i+1) = cos(pos / 10000^{2i/d_{model}})$ </span>\n",
    "3. **Multi-Head Attention**\n",
    "    - Concept and advantages\n",
    "    - Mathematical derivation: <span style=\"font-size: 0.7em\"> $MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O$ </span>\n",
    "4. **Feedforward Networks and Normalization**\n",
    "    - Layer details: <span style=\"font-size: 0.7em\">$FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$</span>\n",
    "    - Role in stabilizing training: <span style=\"font-size: 0.7em\">$LayerNorm(x + Sublayer(x))$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Mathematical Breakdown of Transformers\n",
    "\n",
    "\"In transformers, the self-attention mechanism allows each word to consider all other words in a sequence. Given a sequence of input vectors X, the self-attention mechanism computes queries, keys and values.\n",
    "The attention scores are derived by taking the dot product of queries and keys, scaling by the square root of the dimensionality , and applying the softmax function. These scores are then multiplied with the values.'\n",
    "\n",
    "For example: For a sequence like Patient has fever, we calculate queries, keys, and values from the input sequence and then determine the attention scores to obtain a weighted sum of the values, reflecting the importance of each word in the context of others.\"\n",
    "\n",
    "\n",
    "\"Transformers lack inherent order information, so positional encoding is used to introduce the sequence order. The positional encoding is calculated as: For even indices, the position encoding is the sine of the position divided by ten thousand raised to the power of two i over the model’s dimension. For odd indices, we use the cosine function.'\n",
    "\n",
    "For instance: In a clinical note, this encoding helps in maintaining the positional context of each word.\"\n",
    "\n",
    "\n",
    "\"Multi-head attention allows the model to focus on different aspects of the input sequence. The outputs of several attention heads are concatenated and then multiplied by a weight matrix w0, enabling the model to capture various relationships within the data simultaneously.\"\n",
    "\n",
    "Feedforward Networks and Normalization\n",
    "\n",
    "Each layer of the transformer includes a feedforward network and layer normalization. The feedforward layer applies a linear transformation to x using weight matrix W1 and bias b1, followed by a ReLU activation function, and another linear transformation using weight matrix W2 and bias b2.\n",
    "\n",
    "Layer Normalization ensures stability in training and is expressed as Layer normalization is applied to the sum of the input x and the output from the sublayer, enhancing gradient flow and training stability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Section 5: Training and Implementation\n",
    "1. **Training Process**\n",
    "    - Data preprocessing (tokenization, embedding)\n",
    "    $$\\text{The patient is healthy.} \\rightarrow [12, 45, 23, 67]$$\n",
    "    - Loss functions (cross-entropy)\n",
    "    - Optimization algorithms (Adam, learning rate schedules)\n",
    "2. **Python Implementation**\n",
    "    - Code snippets for key components\n",
    "    - Building a simple Transformer model\n",
    "    - Example: Training on clinical note data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Training a transformer model involves several key steps like data preprocessing, defining loss functions, and selecting optimization algorithms. In Python, we can implement these steps using libraries like TensorFlow or PyTorch.\n",
    "\n",
    "Data Preprocessing involves tokenization and embedding. Tokenization step splits the text into smaller pieces called tokens, which can be words or subwords. For example, the sentence 'The patient is healthy' might be split into ['The', 'patient', 'is', 'healthy']. During embedding each token is mapped to a high-dimensional vector. These vectors represent the tokens in a way that captures their meanings and relationships in the context of the entire text.\"\n",
    "\n",
    "\"We use cross-entropy loss to measure the difference between the predicted distribution of words and the true distribution. This loss function helps us quantify how well the model is performing by comparing the predicted output with the actual data.\"\n",
    "\n",
    "\"The Adam optimizer, which stands for Adaptive Moment Estimation, is commonly used in training transformers. It adjusts the learning rate for each parameter dynamically, combining the benefits of two other algorithms: AdaGrad and RMSProp. This results in faster convergence and more efficient training.\"\n",
    "\n",
    "In the following slide we provide code snippets for key components of the transformer model, including multi-head attention, positional encoding, and feedforward networks. We also demonstrate how to build a simple transformer model in Python and train it on clinical note data.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert d_model % num_heads == 0\n",
    "        self.depth = d_model // num_heads\n",
    "        self.Wq = nn.Linear(d_model, d_model)\n",
    "        self.Wk = nn.Linear(d_model, d_model)\n",
    "        self.Wv = nn.Linear(d_model, d_model)\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        batch_size = q.size(0)\n",
    "        q = self.split_heads(self.Wq(q), batch_size)\n",
    "        k = self.split_heads(self.Wk(k), batch_size)\n",
    "        v = self.split_heads(self.Wv(v), batch_size)\n",
    "        attn_output, _ = scaled_dot_product_attention(q, k, v)\n",
    "        attn_output = attn_output.permute(0, 2, 1, 3).contiguous()\n",
    "        attn_output = attn_output.view(batch_size, -1, self.d_model)\n",
    "        output = self.dense(attn_output)\n",
    "        return output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v):\n",
    "    matmul_qk = torch.matmul(q, k.transpose(-2, -1))\n",
    "    dk = q.size()[-1]\n",
    "    scaled_attention_logits = matmul_qk / math.sqrt(dk)\n",
    "    attention_weights = torch.nn.functional.softmax(scaled_attention_logits, dim=-1)\n",
    "    output = torch.matmul(attention_weights, v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This code demonstrates the Multi-Head Attention mechanism in transformers using PyTorch. torch is the main library for tensor operations, and torch.nn provides neural network modules.\n",
    "\n",
    "The code defines a MultiHeadAttention class that inherits from nn.Module. It sets up linear layers for queries, keys, values, and a dense layer. The split_heads method reshapes the tensor for multiple attention heads, and the forward method projects inputs into queries, keys, and values, applies attention, and combines the outputs.\n",
    "\n",
    "\n",
    "scaled_dot_product_attention Function Computes attention scores by scaling the dot product of queries and keys, applying softmax to get attention weights, and then using these to get a weighted sum of values.\n",
    "This mechanism allows transformers to handle different parts of the input simultaneously, improving parallel processing and performance.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, max_pos_encoding, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, max_pos_encoding, rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, max_pos_encoding, rate)\n",
    "        self.final_layer = nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "    def forward(self, inp, tar):\n",
    "        enc_output = self.encoder(inp)\n",
    "        dec_output, _ = self.decoder(tar, enc_output)\n",
    "        final_output = self.final_layer(dec_output)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This code defines a simple Transformer model using PyTorch, incorporating both an encoder and a decoder. Transformer class Inherits from nn.Module and initializes the encoder, decoder, and final linear layer.\n",
    "Constructor Initializes the transformer with the specified number of layers, model dimension (d_model), number of attention heads (num_heads), feedforward dimension (dff), vocabulary sizes for input and target, and positional encoding length. A dropout rate (rate) can also be set.\n",
    "Encoder Handles the input sequence, encoding it into a high-dimensional representation.\n",
    "Decoder Takes the encoded input and processes the target sequence, generating an output sequence.\n",
    "Final Layer A linear layer that maps the decoder’s output to the target vocabulary size.\n",
    "forward Method Defines the forward pass through the model. It passes the input sequence through the encoder, uses the encoder output to inform the decoding of the target sequence, and produces the final output. This simple transformer structure demonstrates the core components of a transformer: encoding the input, decoding the target, and producing the final output.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Assume `clinical_notes` is a list of clinical notes and `labels` are the corresponding targets\n",
    "# Preprocess the data\n",
    "# ...\n",
    "\n",
    "# Create the transformer model\n",
    "model = Transformer(num_layers=2, d_model=512, num_heads=8, dff=2048, input_vocab_size=8000, target_vocab_size=8000, max_pos_encoding=5000)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch, (inp, tar) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inp, tar)\n",
    "        loss = criterion(output, tar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This example demonstrates how to train a Transformer model on clinical note data using PyTorch. The training process involves data preprocessing, model instantiation, defining loss functions, and optimization algorithms.\n",
    "\n",
    "Prepare clinical_notes and labels as input and target data. This step typically involves tokenizing and encoding the text into numerical format. Then create the Transformer Model using the Transformer class defined earlier. We instantiate a transformer with 2 layers, a model dimension (d_model) of 512, 8 attention heads, a feedforward dimension (dff) of 2048, input and target vocabulary sizes of 8000, and a maximum positional encoding length of 5000. We then define the loss function as CrossEntropyLoss and the optimizer as Adam, which adjusts the learning rate dynamically. The model is trained using a simple training loop that iterates over the data for a specified number of epochs. For each batch, the model computes the loss, backpropagates the gradients, and updates the parameters. The training loop prints the loss at each epoch to track training progress. This example shows the basic steps to train a transformer model on clinical note data, highlighting data preprocessing, model instantiation, and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Section 6: Applications and Advancements\n",
    "1. **Applications of Transformer Models**\n",
    "    - Clinical notes processing\n",
    "    - Other NLP tasks (translation, summarization, etc.)\n",
    "2. **Recent Advancements**\n",
    "    - Variants of Transformer models (BERT, GPT)\n",
    "    - Future directions and research areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Applications of Transformer Models\n",
    "\n",
    "\"Transformers have transformed many NLP tasks with their ability to handle complex language processing efficiently. Some applications include:\n",
    "\n",
    "Clinical Notes Processing: They are used to extract key medical information from clinical notes and summarize patient histories effectively.\n",
    "Other NLP Tasks: Transformers excel in translation, summarization, and question answering, among other applications.\"\n",
    "\n",
    "Recent Advancements include:\n",
    "Variants of Transformer Models like BERT and GPT, which have further improved language understanding and generation capabilities.\n",
    "\n",
    "BERT: Stands for Bidirectional Encoder Representations from Transformers. It understands the context of a word in all directions by looking at the entire sequence.\n",
    "GPT: Short for Generative Pre-trained Transformer. It generates coherent and contextually relevant text based on the input prompt.\n",
    "\n",
    "Future Directions and Research Areas include:\n",
    "\n",
    "Continual Learning: Adapting models to new data and tasks over time without forgetting previous knowledge.\n",
    "Multimodal Transformers: Integrating text with other data types like images and audio for richer context understanding.\n",
    "Efficient Transformers: Developing models that require less computational power and memory, making them more accessible.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "1. **Recap and Key Takeaways**\n",
    "2. **Q&A Session**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "To summarize, transformers have revolutionized the field of language modeling by overcoming the limitations of previous architectures like RNNs and LSTMs. They have enabled significant advancements in natural language processing, including better handling of long-range dependencies and efficient parallel processing. This has led to improved performance in tasks such as translation, summarization, and understanding context in language.\n",
    "\n",
    "Finally, I’d like to open the floor for any questions you might have. Feel free to ask about any aspect of transformers, their applications, or the advancements we discussed. Thank you for your attention, and I hope you found this session informative and engaging!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
