{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parallelism and GPU computing\n",
        "\n",
        "AI calculations involve quite a few arithmetic calculations. Graphics\n",
        "processing units (GPUs) are computer chips that were designed to\n",
        "parallelize large numbers of small arithmetic calculations. They were\n",
        "designed as such because computer graphics involve rotations, shifts,\n",
        "scaling, etc. of images, are matrix or tensor manipulations, i.e. a\n",
        "large collection of small arithmetic calculations.\n",
        "\n",
        "This discussion is relevant, since there is a calculus of\n",
        "parallelization on whether or not it will be worth the effort. In\n",
        "fact, in some cases parallelization can slow down computations. For\n",
        "example, parallelizing across a network will only be beneficial if the\n",
        "cumulative cost of the network transfer is less than the cumulative\n",
        "benefit from parallization. For parallelizing lots of small\n",
        "calculations, the data transfer has to be really fast. What could be\n",
        "faster than on the same chip, like a GPU? The effect of GPU\n",
        "paralleziation on AI learning can be quite significant. We give a\n",
        "simple example below where we parallelize the sum of a few thousand\n",
        "numbers, where the acceleration is on the order of 50 to 70%.\n",
        "\n",
        "## Getting started\n",
        "I don't have a GPU in my personal computer. Some options for trying\n",
        "out GPUs include Google Colab and Paperspace to name two examples. I'm\n",
        "doing this example on Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By default, calculations will be on the CPU. You have to actually\n",
        "migrate calculations to the GPU. Here I write the code in such a way\n",
        "that it works on the GPU or CPU depending on whether a GPU is\n",
        "available. It's typical to create a variable called \"device\" that\n",
        "references the GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else :\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here the GPU is called `cuda:0` referencing the CUDA API for GPU\n",
        "computing. CUDA is one framework for NVIDIA GPUs. It is very nice,\n",
        "since pytorch and other software have developed high level\n",
        "interfaces. You'll see how fast it is to incorporate GPU\n",
        "computing. Let's look at the reduction in runtime for a very simple\n",
        "example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time\n",
        "\n",
        "for i in range(10):\n",
        "  test_matrix = torch.randn([100000, 10])\n",
        "  test_matrix_cuda = test_matrix.to(device)\n",
        "\n",
        "  start = time.time()\n",
        "  test_matrix.sum()\n",
        "  end = time.time()\n",
        "\n",
        "  a = end - start\n",
        "\n",
        "  start = time.time()\n",
        "  test_matrix_cuda.sum()\n",
        "  end = time.time()\n",
        "\n",
        "  b = end - start\n",
        "\n",
        "  print(\"The % reduction in runtime is: \", end = \"\")\n",
        "  print(np.round((1 - b / a) * 100, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Redoing our GAN example\n",
        "\n",
        "Let's redo our GAN example using GPU acceleration. We won't even use random batches, since it runs so much faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "import PIL\n",
        "\n",
        "## Read in and organize the data\n",
        "imgURL = \"https://raw.githubusercontent.com/larvalabs/cryptopunks/master/punks.png\"\n",
        "urllib.request.urlretrieve(imgURL, \"cryptoPunksAll.jpg\")\n",
        "img = PIL.Image.open(\"cryptoPunksAll.jpg\").convert(\"RGB\")\n",
        "imgArray = np.asarray(img)\n",
        "finalArray = np.empty((10000, 3, 24, 24))\n",
        "for i in range(100):\n",
        "  for j in range(100):\n",
        "    a, b = 24 * i, 24 * (i + 1)  \n",
        "    c, d = 24 * j, 24 * (j + 1) \n",
        "    idx = j + i * (100)\n",
        "    finalArray[idx,0,:,:] = imgArray[a:b,c:d,0]\n",
        "    finalArray[idx,1,:,:] = imgArray[a:b,c:d,1]\n",
        "    finalArray[idx,2,:,:] = imgArray[a:b,c:d,2]\n",
        "\n",
        "n = finalArray.shape[0]\n",
        "x_real = finalArray / 255\n",
        "x_real = torch.tensor(x_real.astype(np.float32)).to(device)\n",
        "kernel_size = 5\n",
        "generator_input_dim = [16, 3, 3]\n",
        "\n",
        "class create_generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()        \n",
        "        self.net = nn.Sequential(\n",
        "            nn.ConvTranspose2d(16, 128, 10, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 3, 4, 2, 1, bias=False), \n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        " \n",
        "## Use the discriminator from the convnet chapter\n",
        "class create_discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 12, 5)\n",
        "        self.fc1 = nn.Linear(12 * 3 * 3, 32)\n",
        "        self.fc2 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "    \n",
        "        \n",
        "generator = create_generator().to(device)\n",
        "discriminator = create_discriminator().to(device)\n",
        "\n",
        "lr = 1e-4\n",
        "\n",
        "## y is n real images then n fake images\n",
        "y = torch.concat( (torch.ones(n), torch.zeros(n) ) ).to(device)\n",
        "## Note the outcome for the generator is all ones even\n",
        "## though we're classifying real as 1 and fake as 0\n",
        "## In other words, we want the loss for the generator to be\n",
        "## based on how real-like the generated data is\n",
        "y_fake = torch.ones( (n, 1) ).to(device)\n",
        "\n",
        "## Set up optimizers\n",
        "optimizerD = optim.Adam(discriminator.parameters(), lr=lr)\n",
        "optimizerG = optim.Adam(generator.parameters(), lr=lr)\n",
        "\n",
        "## Set up the loss function\n",
        "loss_function = nn.BCELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## I want an animation\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML, Image\n",
        "\n",
        "## Create a vector to keep track of\n",
        "z = torch.randn([1]+generator_input_dim, device = device)\n",
        "## Animation stuff\n",
        "fig, ax = plt.subplots()\n",
        "ims = []\n",
        "plt.close(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "n_epochs = 4000\n",
        "\n",
        "for epoch in range(n_epochs):          \n",
        "    ## Generate the simulated embedding    \n",
        "    embedding = torch.randn([n]+generator_input_dim, device = device)    \n",
        "\n",
        "    ## Generate new fake images\n",
        "    x_fake = generator(embedding)\n",
        "    \n",
        "    ######################## train the discriminator\n",
        "    ## zero out the gradient\n",
        "    discriminator.zero_grad()\n",
        "\n",
        "    ## run the generated and fake images through the discriminator\n",
        "    yhat_fake = discriminator(x_fake.detach())\n",
        "    yhat_real = discriminator(x_real)\n",
        "\n",
        "    ## Note you have to concatenate them in the same order as \n",
        "    ## the previous cell. Remember we did real then fake\n",
        "    yhat = torch.concat( (yhat_real, yhat_fake) ).reshape(-1)\n",
        "\n",
        "    discriminator_error = loss_function(yhat, y)\n",
        "\n",
        "    # Calculate gradients for D in backward pass\n",
        "    discriminator_error.backward(retain_graph = True)\n",
        "\n",
        "    # Update the discriminator\n",
        "    optimizerD.step()\n",
        "\n",
        "    ############### Train the generator\n",
        "\n",
        "    ## zero out the gradient\n",
        "    generator.zero_grad()\n",
        "\n",
        "    ## The discriminator has been udpated, so push the data through the \n",
        "    ## new discriminator\n",
        "    yhat_fake = discriminator(x_fake)\n",
        "    \n",
        "    generator_error = loss_function( yhat_fake,  y_fake )\n",
        "\n",
        "    ## Calculate the backwards error\n",
        "    generator_error.backward(retain_graph = False)\n",
        "\n",
        "    # Update the discriminator\n",
        "    optimizerG.step()\n",
        "    \n",
        "    if (epoch + 1) % 10 == 0:\n",
        "      x_temp = generator(z).detach().cpu().numpy()[0, :, :, :]\n",
        "      img = np.transpose(x_temp, (1, 2, 0))\n",
        "      im = ax.imshow(img, animated=True)\n",
        "      ims.append([im])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True,repeat_delay=1000)\n",
        "HTML(ani.to_html5_video())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25): \n",
        "  plt.subplot(5, 5,i+1)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  img = np.transpose(x_fake.detach().cpu().numpy()[i,:,:,:], (1, 2, 0))\n",
        "  plt.imshow(img)  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}