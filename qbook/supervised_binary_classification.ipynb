{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to binary classification\n",
        "\n",
        "The data we're going to be working with for this example is the\n",
        "`oasis` data from Elizabeth Sweeney's R package. The data contain MR\n",
        "(magnetic resonance) images for lesion segmentation in multiple\n",
        "sclerosis (MS). MS is a disorder primarily caused by whtie matter\n",
        "lesions. This dataset is a collection of voxels from an image with\n",
        "radiologists labeling of whether or not a white matter lesion exists\n",
        "at that location."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "## this sets some style parameters\n",
        "sns.set()\n",
        "from sklearn.metrics import accuracy_score, roc_curve, auc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dat = pd.read_csv(\"https://raw.githubusercontent.com/bcaffo/ds4bme_intro/master/data/oasis.csv\")\n",
        "dat.head(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that we loaded `pandas` first. The various columns are voxel\n",
        "values from different kinds of MR images of the same patient. FLAIR\n",
        "(fluid attenuated inversion recovery), PD (proton density), T1 and\n",
        "T2. The latter two aren't acronyms, but instead named for the specific\n",
        "part of the *relaxation time* of the MR signal. Roughly, the\n",
        "relaxation time is related to the signal produced by protons snapping\n",
        "back into alignment in a strong magnetic field (recall *magnetic*\n",
        "resonance imaging). The `_10` and `_20` ending variables are local\n",
        "averages of the neighboring voxels. We're trying to predict\n",
        "`GOLD_Lesions`, which is the radiologist standard of whether or not\n",
        "there is a lesion at this voxel. (A voxel is a three dimensional\n",
        "pixel.)\n",
        "\n",
        "Note here we are doing *voxelwise segmentation* that is trying to\n",
        "predict whether there is a lesion at each specific voxel. This can be\n",
        "viewed as an image processing problem. Other classification problem\n",
        "consider, for example, whether a patient has any lesions (and then\n",
        "where as a followup). Approaching the problem that way is a *image\n",
        "level* segmentation approach.\n",
        "\n",
        "Let's plot it. I'm showing a couple of ways. I've been testing out\n",
        "plotting libraries in python, and I think that I like 'seaborn' (the\n",
        "second plot) the best. In the seaborn plots, I show both the marginal\n",
        "plot (without considering the gold standard) and then stratified by\n",
        "whether or not there was a lesion at that voxel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dat.groupby('GOLD_Lesions').FLAIR.hist(alpha= .5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x0 = dat.FLAIR[dat.GOLD_Lesions == 0]\n",
        "x1 = dat.FLAIR[dat.GOLD_Lesions == 1]\n",
        "x2 = dat.FLAIR\n",
        "\n",
        "sns.kdeplot(x2, shade = True, label = 'Marginal')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "sns.kdeplot(x0, shade = True, label = 'Gold Std = 0')\n",
        "sns.kdeplot(x1, shade = True, label = 'Gold Std = 1')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classification\n",
        "\n",
        "Let's try creating the simplest possible classifier, a threshold. So\n",
        "here we want to pick the value of the threshold so that lower values\n",
        "are classified `GOLD_Lesion == 0` (i.e. no lesion) and higher values\n",
        "are `GOLD_Lesion == 1` (lesion at this voxel). We want to do this on\n",
        "labeled voxels so that we can pick a meaningful threshold on voxels\n",
        "without a gold standard labeling. That is, for new patients we want to\n",
        "automatically label their images one voxel at a time with a simple\n",
        "thresholding rule. We're going to use our **training data** where we\n",
        "know the truth to develop the threshold.\n",
        "\n",
        "*Note the idea behind doing this is only useful if the new images\n",
        "without the gold standard are in the same units as the old one, which\n",
        "is not usually the case for MRIs. The technique for trying to make\n",
        "images comparable is called normalization*.\n",
        "\n",
        "Let's first just try each of the datapoints itself as a threshold and\n",
        "pick which one does best.  However, I'm going to break the data into a\n",
        "training and testing set. The reason for this is that I want to make\n",
        "sure that I don't overfit. That is, we're going to test our algorithm\n",
        "on a dataset that wasn't used to train the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x = dat.FLAIR\n",
        "y = dat.GOLD_Lesions\n",
        "n = len(x)\n",
        "trainFraction = .75\n",
        "\n",
        "## Build a training and testing set\n",
        "## Prob of being in the train set is trainFraction\n",
        "sample = np.random.uniform(size = n) < trainFraction\n",
        "\n",
        "## Get the training and testing sets\n",
        "xtrain = x[ sample]\n",
        "ytrain = y[ sample]\n",
        "xtest =  x[~sample]\n",
        "ytest =  y[~sample]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## Starting values, just set it to \n",
        "## 0 so that it improves on the first\n",
        "## try\n",
        "bestAccuracySoFar = 0\n",
        "\n",
        "for t in np.sort(xtrain):\n",
        "  ## Strictly greater than the threshold is\n",
        "  ## our algorithm\n",
        "  predictions = (xtrain > t)\n",
        "  accuracy = np.mean(ytrain == predictions)\n",
        "  if (accuracy > bestAccuracySoFar):\n",
        "    bestThresholdSoFar = t \n",
        "    bestAccuracySoFar = accuracy \n",
        "\n",
        "threshold = bestThresholdSoFar\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's test our our \"algorithm\", on the test set. We'll look at the\n",
        "test set accuracy, but also how it breaks up into the sensisitivity\n",
        "and specificity.\n",
        "\n",
        "## Definitions\n",
        "test set **accuracy** = proportion of correct classifications on the test data\n",
        "\n",
        "test set **sensitivity** = proportion declared diseased among those that are actually diseased. (In this case lesion = disease)\n",
        "\n",
        "test set **specificity** = proportion declared not diseased among those that are actually not diseased.\n",
        "\n",
        "To interpret the sensitivity and specificity, imagine setting the\n",
        "threshold nearly to zero. Then we'll declare almost every voxel a\n",
        "lesion and we'll have nearly 100% sensitivity and nearly 0%\n",
        "specificity. If we declare a voxel as a lesion it's not that\n",
        "interesting. If we declare a voxel as not lesions, then it's probably\n",
        "not a lesion.\n",
        "\n",
        "If we set the threshold really high, then we'll have nearly 0%\n",
        "sensitivity and 100% specificity. If we say a voxel is not lesioned,\n",
        "it's not that informative, since we declare nearly everything not a\n",
        "lesion. But if we declare a voxel a lesion, it usually is.\n",
        "\n",
        "\n",
        "So, if you have a high sensitivity, it's good for ruling diseases\n",
        "out. If you have a high specificity it's good for ruling diseases\n",
        "in. If you have a high both? Then you have a very good test.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## Let's test it out on the test set\n",
        "testPredictions = (xtest > threshold)\n",
        "\n",
        "## The test set accuracy\n",
        "testAccuracy = np.mean(testPredictions == ytest)\n",
        "\n",
        "## Let's see how it specifically does on the\n",
        "## set of instances where ytest == 0 and ytest == 1\n",
        "## The % it gets correct on ytest == 0 is called\n",
        "## the specificity and the percent correct when \n",
        "## ytest == 1 is called the sensitivity.\n",
        "sub0 = ytest == 0\n",
        "sub1 = ytest == 1\n",
        "\n",
        "testSpec = np.mean(ytest[sub0] == testPredictions[sub0])\n",
        "testSens = np.mean(ytest[sub1] == testPredictions[sub1])\n",
        "\n",
        "pd.DataFrame({\n",
        " 'Threshold': threshold,\n",
        " 'Accuracy': testAccuracy, \n",
        " 'Specificity': testSpec, \n",
        " 'Sensitivity': testSens}, index = [0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sns.kdeplot(x0, shade = True, label = 'Gold Std = 0')\n",
        "sns.kdeplot(x1, shade = True, label = 'Gold Std = 1')\n",
        "plt.axvline(x=threshold)\n",
        "            \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OK, so out plot has better sensitivity than specificity and a test set\n",
        "accuracy of around 68%. The lower specificity is because there's a\n",
        "lower percentage of blue below the line than orange above the\n",
        "line. Recall, we're saying above the threshold is a lesion and orange\n",
        "is the distribution for voxels with lesions.\n",
        "\n",
        "So, for this algorithm, the high sensitivity says that all else being\n",
        "equal, if you declare a voxel as not being a lesion, it probably\n",
        "isn't. In other words, if you're out in the lower part of the orange\n",
        "distribution, there's a lot of blue there.\n",
        "\n",
        "However, all else isn't equal. Most voxels aren't lesions. This\n",
        "factors into our discussion in a way that we'll discuss later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fpr, tpr, thresholds = roc_curve(ytest, xtest)\n",
        "roc_auc = auc(fpr, tpr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange',\n",
        "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic example')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}