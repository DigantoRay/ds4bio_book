{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RNNs\n",
        "\n",
        "Recurrent neural networks (RNNs) are commonly used for ordered data, such as in time series analysis or text processing.  \n",
        "\n",
        "It might be worth considering some existing common time series models first.\n",
        "\n",
        "### Linear time sieres models\n",
        "\n",
        "#### Stochastic time series models\n",
        "\n",
        "Perhaps the most popular version of stochastic time series models is the autoregressive (AR) models. Here's an AR(1) model\n",
        "$$\n",
        "Y_t = \\beta_0 + Y_{t-1}\\rho + \\epsilon_t\n",
        "$$\n",
        "\n",
        "Generalizations allow for longer lags, called AR(p) models where p is the number of lags. We could instead have the error terms have dependence in so called moving average (MA) models. Here's an MA(1)\n",
        "\n",
        "$$\n",
        "Y_t = \\beta_0 + Y_{t-1}\\rho + \\epsilon_t + \\gamma \\epsilon_{t-1}\n",
        "$$\n",
        "where, again, longer lags can be incorporated. We could combine these models in ARMA models. Here's an ARMA(1,1) model\n",
        "\n",
        "$$\n",
        "Y_t = \\beta_0 + Y_{t-1}\\rho + \\epsilon_t + \\gamma \\epsilon_{t-1}\n",
        "$$\n",
        "\n",
        "\n",
        "RNNs get their name since the hidden layers point to themselves. This is already well explored in time series analyses, where we can have models such as AR models:\n",
        "\n",
        "Differencing, i.e. considering $Y_t - Y_{t-1}$ can be thought of as looking at a linear approximation to the derivative of the $Y_t$ process. Second order differencing simply looks at $(Y_t - Y_{t-1}) - (Y_{t-1} - Y_{t-2})$ is then approximating the second derivative and so on. ARIMA models look at ARMA models on differenced data. So, an ARIMA model can be specified with three numbers the AR part, the MA part and the differencing part.\n",
        "\n",
        "Stochastic models are especially useful in things like finance for modeling asset prices. This is because efficient market theory suggests no explanatory variables are needed so looking at the time series and so modeling the data this way is often useful. \n",
        "\n",
        "#### Functional outcomes\n",
        "\n",
        "Instead of modeling the outcome time series as a stochastic process, we might be interested in modeling it as a function using a smoother. For example we might model\n",
        "$$\n",
        "Y_t = f(t) + \\epsilon = z_t \\beta + \\epsilon\n",
        "$$\n",
        "where $z_t$ is a basis element from a smoother matrix. An example is regression splines.\n",
        "\n",
        "#### Explanatory time series models\n",
        "\n",
        "Consider regression models for $Y ~|~ X$ where $Y$ and $X$ are time series. We might consider concordant models\n",
        "\n",
        "$$\n",
        "Y_t = \\beta_0 + \\beta_1 X_t + \\epsilon_t\n",
        "$$\n",
        "\n",
        "Distributed lag models\n",
        "$$\n",
        "Y_t = \\alpha + \\beta_0 X_t + \\beta_1 X_{t-1} + \\ldots + \\beta_p X_{t-p} + \\epsilon_t\n",
        "$$\n",
        "\n",
        "Markov models model $Y_t ~|~ Y_{t-1}, \\mbox{Other covariates}$. So, we can model\n",
        "$$\n",
        "Y_t = \\alpha + \\beta_0 Y_{t-1} + \\epsilon_t.\n",
        "$$\n",
        "\n",
        "Not unlike the MA models, in all of these cases we can model dependence in the $\\epsilon$ terms. \n",
        "\n",
        "#### Functional predictors\n",
        "\n",
        "Consider the instance where $Y$ is not a time series, but $X_{i}$ is for each $i$. Let's write this as a function, $x_i(t)$. We then need to relate an entire time series to each $Y$ value. We might consider models of the form\n",
        "$$\n",
        "Y_i = \\alpha + \\int \\beta(t) x_i(t) dt + \\epsilon_i \n",
        "$$\n",
        "\n",
        "#### Summarizing linear time series models\n",
        "\n",
        "(base) bcaffo@penguin:~/sandboxes/advanced_ds4bio_book$ \n",
        "In summary, there's quite a few ways to model time series data lineary. And you can combine methods and extend them to generalized linear model settings. \n",
        "\n",
        "### RNNS\n",
        "\n",
        "RNNs have many variations, the same as linear time series models. They are called recurrent, since they point to themselves. Let's look at a simple RNN where we have a time series, $X_{it}$ and we want to predict a concordant time series, $y_t$. Consider a model $h_t = \\mathrm{expit}(w_0 + w_1 h_{t-1} + w_2 x_{t})$ and $\\hat Y_t = \\mathrm{expit}(w_3 + w_4 h_t)$ [@elman1990finding].\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sklearn as skl\n",
        "\n",
        "plt.figure(figsize=[6, 6])\n",
        "G = nx.DiGraph()\n",
        "\n",
        "G.add_node(\"X(t-1)\",  pos = (0  ,   1)   )\n",
        "G.add_node(\"X(t)\"  ,  pos = (0.5  ,   1) )\n",
        "G.add_node(\"X(t+1)\",  pos = (1  ,   1)   ) \n",
        "\n",
        "G.add_node(\"h(t-1)\",  pos = (0,   0.5) )\n",
        "G.add_node(\"h(t)\"  ,  pos = (0.5, 0.5) )\n",
        "G.add_node(\"h(t+1)\",  pos = (1,   0.5) )\n",
        "\n",
        "G.add_node(\"Y(t-1)\",  pos =  (0 ,   0) )\n",
        "G.add_node(\"Y(t)\"  ,  pos =  (0.5 ,   0) )\n",
        "G.add_node(\"Y(t+1)\",  pos =  (1 ,   0) )\n",
        "\n",
        "G.add_edges_from([\n",
        "    ['X(t-1)',  'h(t-1)'],\n",
        "    ['X(t)', 'h(t)'],\n",
        "    ['X(t+1)', 'h(t+1)'],\n",
        "    ['h(t-1)', 'h(t)'],\n",
        "    ['h(t)', 'h(t+1)'],\n",
        "    ['h(t-1)', 'Y(t-1)'],\n",
        "    ['h(t)', 'Y(t)'],\n",
        "    ['h(t+1)', 'Y(t+1)'] \n",
        "] )\n",
        "\n",
        "nx.draw(G, \n",
        "        nx.get_node_attributes(G, 'pos'), \n",
        "        with_labels=True, \n",
        "        font_weight='bold', \n",
        "        node_size = 4000,\n",
        "        node_color = \"lightblue\",\n",
        "        linewidths = 3)\n",
        "ax= plt.gca()\n",
        "ax.collections[0].set_edgecolor(\"#000000\")\n",
        "ax.set_xlim([-.3, 1.3])\n",
        "ax.set_ylim([-.3, 1.3])\n",
        "\n",
        "\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice, the prior hidden nodes point to the subsequent nodes. This builds in the history of the network.\n",
        "\n",
        "\n",
        "We can also have networks that point to single outcomes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "plt.figure(figsize=[6, 6])\n",
        "G = nx.DiGraph()\n",
        "\n",
        "G.add_node(\"X(t-1)\",  pos = (0  ,   1)   )\n",
        "G.add_node(\"X(t)\"  ,  pos = (0.5  ,   1) )\n",
        "G.add_node(\"X(t+1)\",  pos = (1  ,   1)   ) \n",
        "\n",
        "G.add_node(\"h(t-1)\",  pos = (0,   0.5) )\n",
        "G.add_node(\"h(t)\"  ,  pos = (0.5, 0.5) )\n",
        "G.add_node(\"h(t+1)\",  pos = (1,   0.5) )\n",
        "\n",
        "G.add_node(\"Y\",  pos =  (1 ,   0) )\n",
        "\n",
        "G.add_edges_from([\n",
        "    ['X(t-1)',  'h(t-1)'],\n",
        "    ['X(t)', 'h(t)'],\n",
        "    ['X(t+1)', 'h(t+1)'],\n",
        "    ['h(t-1)', 'h(t)'],\n",
        "    ['h(t)', 'h(t+1)'],\n",
        "    ['h(t+1)', 'Y'] \n",
        "] )\n",
        "\n",
        "nx.draw(G, \n",
        "        nx.get_node_attributes(G, 'pos'), \n",
        "        with_labels=True, \n",
        "        font_weight='bold', \n",
        "        node_size = 4000,\n",
        "        node_color = \"lightblue\",\n",
        "        linewidths = 3)\n",
        "ax= plt.gca()\n",
        "ax.collections[0].set_edgecolor(\"#000000\")\n",
        "ax.set_xlim([-.3, 1.3])\n",
        "ax.set_ylim([-.3, 1.3])\n",
        "\n",
        "\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example character prediction\n",
        "\n",
        "I used [this tutorial](https://edumunozsala.github.io/BlogEms/fastpages/jupyter/rnn/lstm/pytorch/2020/09/03/char-level-text-generator-pytorch.html) by [Eduardo Mu√±oz](https://edumunozsala.github.io/BlogEms/about/) and the [pytorch character prediction tutorial](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html) here. \n",
        "\n",
        "Let's predict the last letter of words from Shakespeare's writing from the previous words. Using this, you should be able to extrapolate how to create a character prediction algorithm and from that a word prediction algorithm.\n",
        "\n",
        " I got the training text from [here](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt) with a simple `url` request. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import string\n",
        "import nltk\n",
        "from urllib import request"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "urlresponse = request.urlopen(url)\n",
        "text = urlresponse.read().decode(\"ascii\")\n",
        "text_words = nltk.tokenize.word_tokenize(text.lower())\n",
        "\n",
        "##remove all of the non alpha characters like commas, periods ...\n",
        "text_words = [word for word in text_words if word.isalpha() and len(word) > 2]\n",
        "\n",
        "## Test whether all 26 letters are represented\n",
        "#text_letters = set([l.lower() for l in text if l.isalpha()])\n",
        "#len(text_letters)\n",
        "## All characters are represented\n",
        "\n",
        "## The lowercase letters as a list\n",
        "import string\n",
        "letters = string.ascii_lowercase\n",
        "n_letters = len(letters)\n",
        "\n",
        "\n",
        "## one hot encode each letter then create a matrix for each word\n",
        "def word_encode(word):\n",
        "    n_word = len(word)\n",
        "    input_tensor = torch.zeros(len(word) - 1, 1, n_letters)\n",
        "    for i in range(n_word - 1):\n",
        "        l = word[i]\n",
        "        input_tensor[i, 0, letters.find(l)] = 1\n",
        "    output_category = letters.find(word[i + 1])\n",
        "    return input_tensor, output_category\n",
        "\n",
        "\n",
        "\n",
        "test_word = text_words[0]\n",
        "test_predictor, test_outcome = word_encode(test_word)\n",
        "print(test_word)\n",
        "print(test_predictor.shape)\n",
        "print(test_outcome)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's create a list of our predictor tensor and outcome categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "N = len(text_words)\n",
        "predictor_list = []\n",
        "outcome_list = []\n",
        "for i in range(N):\n",
        "    w = text_words[i]\n",
        "    p, o = word_encode(w)\n",
        "    predictor_list.append(p)\n",
        "    outcome_list.append(o)\n",
        "\n",
        "outcome_tensor = torch.tensor(outcome_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's the RNN from their tutorial "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        combined = torch.cat( (input, hidden), 1)\n",
        "        hidden = self.i2h(combined)\n",
        "        output = self.i2o(combined)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)\n",
        "\n",
        "n_hidden = 256\n",
        "rnn = RNN(n_letters, n_hidden, n_letters)\n",
        "\n",
        "test_hidden = rnn.initHidden()\n",
        "test_input_val = test_predictor[0]\n",
        "print(test_input_val.shape)\n",
        "print(rnn.forward(test_input_val, test_hidden))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "learning_rate = 1e-4\n",
        "epochs = 1000 \n",
        "batch_size = 100\n",
        "\n",
        "optimizer = optim.Adam(rnn.parameters(),  lr = learning_rate)\n",
        "\n",
        "## This runs the first few characters of a word\n",
        "## through the RNN\n",
        "def predict(input_tensor):\n",
        "    prompt_length = input_tensor.size()[0]\n",
        "    h = rnn.initHidden()\n",
        "    for j in range(prompt_length):\n",
        "        o, h = rnn(input_tensor[j], h)\n",
        "    return o, h\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    ## grab a random batch by grabbing random indices\n",
        "    batch_indices = np.random.choice(N, batch_size)\n",
        "\n",
        "    ## initialize the predictions\n",
        "    output = torch.zeros(batch_size, n_letters)\n",
        "    ## run through the batch\n",
        "    for i in range(batch_size):\n",
        "        index = batch_indices[i]\n",
        "        input_tensor = predictor_list[index]\n",
        "        o, h = predict(input_tensor)\n",
        "        output[i,:] = o\n",
        "\n",
        "    rnn.zero_grad()\n",
        "    loss = criterion(output, outcome_tensor[batch_indices])\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Try some words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## The confusion matrix\n",
        "cm = np.zeros( (n_letters, n_letters) )\n",
        "for i in range(1000):\n",
        "    input_tensor = predictor_list[i]\n",
        "    output = predict(input_tensor)[0]\n",
        "    actual = letters.find(text_words[i][-1])\n",
        "    guess = torch.argmax(output)\n",
        "    cm[actual, guess] += 1\n",
        "\n",
        "plt.imshow(cm)\n",
        "plt.xticks([])\n",
        "plt.yticks([])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "np.sum(np.diag(cm)) / np.sum(cm)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}