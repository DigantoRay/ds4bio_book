{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pytorch by example\n",
        "\n",
        "This example from the pytorch documentation [here](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html) displays generating random y ad x dat and fitting a multi-layer neural network. We're going to consider a regression problem, but using a two layer neural network to solve it. Consider something like this. We have 5 inputs; those get passed to 3 hidden nodes, those get RELU'd, which get passed to an output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "hide-input"
        ]
      },
      "source": [
        "#| echo: false\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sklearn as skl\n",
        "\n",
        "#plt.figure(figsize=[2, 2])\n",
        "G = nx.DiGraph()\n",
        "G.add_node(\"X1\",  pos = (0, 2) )\n",
        "G.add_node(\"X2\",  pos = (1, 2) )\n",
        "G.add_node(\"X3\",  pos = (2, 2) )\n",
        "G.add_node(\"X4\",  pos = (3, 2) )\n",
        "G.add_node(\"X5\",  pos = (4, 2) )\n",
        "\n",
        "G.add_node(\"H1\",  pos = (1, 1) )\n",
        "G.add_node(\"H2\",  pos = (2, 1) )\n",
        "G.add_node(\"H3\",  pos = (3, 1) )\n",
        "G.add_node(\"Y\" ,  pos = (2, 0) )\n",
        "\n",
        "G.add_edges_from([ (\"X1\", \"H1\"), (\"X2\", \"H1\"), (\"X3\", \"H1\"),  (\"X4\", \"H1\"), (\"X5\", \"H1\")])\n",
        "G.add_edges_from([ (\"X1\", \"H2\"), (\"X2\", \"H2\"), (\"X3\", \"H2\"),  (\"X4\", \"H2\"), (\"X5\", \"H2\")])\n",
        "G.add_edges_from([ (\"X1\", \"H3\"), (\"X2\", \"H3\"), (\"X3\", \"H3\"),  (\"X4\", \"H3\"), (\"X5\", \"H3\")])\n",
        "G.add_edges_from([ (\"H1\",  \"Y\"), (\"H2\",  \"Y\"), (\"H3\", \"Y\")])\n",
        "nx.draw(G, \n",
        "        nx.get_node_attributes(G, 'pos'), \n",
        "        with_labels=True, \n",
        "        font_weight='bold', \n",
        "        node_size = 2000,\n",
        "        node_color = \"lightblue\",\n",
        "        linewidths = 3)\n",
        "ax= plt.gca()\n",
        "ax.collections[0].set_edgecolor(\"#000000\")\n",
        "ax.set_xlim([-.3, 4.3])\n",
        "ax.set_ylim([-.3, 2.3])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below, we create an example. It isn't terribly interesting, since the X and the Y aren't related at all. But, it does show us some useful code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.linear_model as lm\n",
        "sns.set()\n",
        "\n",
        "\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 1000, 128, 32, 8\n",
        "\n",
        "# Create random Tensors to hold inputs and outputs\n",
        "x = torch.randn(N, D_in)\n",
        "y = torch.randn(N, D_out)\n",
        "\n",
        "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
        "# is a Module which contains other Modules, and applies them in sequence to\n",
        "# produce its output. Each Linear Module computes output from input using a\n",
        "# linear function, and holds internal Tensors for its weight and bias.\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(D_in, H),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(H, D_out),\n",
        ")\n",
        "\n",
        "# The nn package also contains definitions of popular loss functions; in this\n",
        "# case we will use Mean Squared Error (MSE) as our loss function.\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "learning_rate = 1e-4\n",
        "for t in range(1000):\n",
        "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
        "    # override the __call__ operator so you can call them like functions. When\n",
        "    # doing so you pass a Tensor of input data to the Module and it produces\n",
        "    # a Tensor of output data.\n",
        "    y_pred = model(x)\n",
        "\n",
        "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
        "    # values of y, and the loss function returns a Tensor containing the\n",
        "    # loss.\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # Zero the gradients before running the backward pass.\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
        "    # parameters of the model. Internally, the parameters of each Module are stored\n",
        "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
        "    # all learnable parameters in the model.\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
        "    # we can access its gradients like we did before.\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            param -= learning_rate * param.grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's update that example for our setting using the voxel level data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## Read in the data and display a few rows\n",
        "dat = pd.read_csv(\"https://raw.githubusercontent.com/bcaffo/ds4bme_intro/master/data/oasis.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "G = nx.DiGraph()\n",
        "G.add_node(\"PD\",     pos = (0, 2) )\n",
        "G.add_node(\"T1\",     pos = (1, 2) )\n",
        "G.add_node(\"T2\",     pos = (2, 2) )\n",
        "G.add_node(\"PD_10\",  pos = (3, 2) )\n",
        "G.add_node(\"T1_10\",  pos = (4, 2) )\n",
        "G.add_node(\"T2_10\",  pos = (5, 2) )\n",
        "\n",
        "G.add_node(\"H1\",  pos = (1.0, 1) )\n",
        "G.add_node(\"H2\",  pos = (2.5, 1) )\n",
        "G.add_node(\"H3\",  pos = (4.0, 1) )\n",
        "\n",
        "G.add_node(\"FLAIR\" ,  pos = (2.5, 0) )\n",
        "\n",
        "G.add_edges_from([ (\"PD\", \"H1\"), (\"T1\", \"H1\"), (\"T2\", \"H1\"),  (\"PD_10\", \"H1\"), (\"T1_10\", \"H1\"), (\"T2_10\", \"H1\")])\n",
        "G.add_edges_from([ (\"PD\", \"H2\"), (\"T1\", \"H2\"), (\"T2\", \"H2\"),  (\"PD_10\", \"H2\"), (\"T1_10\", \"H2\"), (\"T2_10\", \"H2\")])\n",
        "G.add_edges_from([ (\"PD\", \"H3\"), (\"T1\", \"H3\"), (\"T2\", \"H3\"),  (\"PD_10\", \"H3\"), (\"T1_10\", \"H3\"), (\"T2_10\", \"H3\")])\n",
        "\n",
        "G.add_edges_from([ (\"H1\",  \"FLAIR\"), (\"H2\",  \"FLAIR\"), (\"H3\", \"FLAIR\")])\n",
        "nx.draw(G, \n",
        "        nx.get_node_attributes(G, 'pos'), \n",
        "        with_labels=True, \n",
        "        font_weight='bold', \n",
        "        node_size = 3000,\n",
        "        node_color = \"lightblue\",\n",
        "        linewidths = 3)\n",
        "ax= plt.gca()\n",
        "ax.collections[0].set_edgecolor(\"#000000\")\n",
        "ax.set_xlim([-.5, 5.5])\n",
        "ax.set_ylim([-.3, 2.3])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's set the training fraction at 75% (and thus the testing fraction at 25%). This gives us relatively little data to fit with. Thus, a relatively simple model makes sense."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "trainFraction = .75\n",
        "\n",
        "sample = np.random.uniform(size = 100) < trainFraction\n",
        "trainingDat = dat[sample]\n",
        "testingDat = dat[~sample]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we need to get the data into a pytorch size and fram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x = torch.from_numpy(dat[['PD','T1', 'T2', 'T1_10', 'T2_10', ]].values)\n",
        "y = torch.from_numpy(dat[['FLAIR']].values)\n",
        "\n",
        "##pytorch wants type as float\n",
        "x = x.float()\n",
        "y = y.float()\n",
        "\n",
        "xtraining = x[sample]\n",
        "xtesting = x[~sample]\n",
        "ytraining = y[sample]\n",
        "ytesting = y[~sample]\n",
        "\n",
        "[\n",
        " xtraining.size(),\n",
        " ytraining.size(),\n",
        " xtesting.size(),\n",
        " ytesting.size(),\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## Define the model\n",
        "## Dimension of the hidden layer\n",
        "H = 3\n",
        "\n",
        "## Number of predictors\n",
        "D_in = xtraining.size()[1]\n",
        "D_out = 1\n",
        "\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(D_in, H),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(H, D_out),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "learning_rate = 1e-4\n",
        "for t in range(100000):\n",
        "    y_pred = model(xtraining)\n",
        "    loss = loss_fn(y_pred, ytraining)\n",
        "    if t % 10000 == 0:\n",
        "        print(t, loss.item())\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            param -= learning_rate * param.grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## try prediction\n",
        "ytesting_pred = model(xtesting)\n",
        "a = ytesting_pred.detach().numpy()\n",
        "\n",
        "plt.scatter(a[:,0], ytesting[:,0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}