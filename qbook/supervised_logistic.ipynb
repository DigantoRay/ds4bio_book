{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Logistic regression\n",
        "## Classification with one continuous variable\n",
        "\n",
        "Suppose now that we want to predict the gold standard from the FLAIR\n",
        "values. Fitting a line seems weird, since the outcome can only be 0\n",
        "or 1. A line would allow for arbitrarily small or large\n",
        "predictions. Similiarly, forcing the prediction to be exactly 0 or 1\n",
        "leads to difficult optimization problems. A clever solution is to\n",
        "instead model\n",
        "\n",
        "$$\n",
        "P(Y_i = 1 ~|~ X_i)\n",
        "$$\n",
        "\n",
        "where $Y_i$ is the gold standard value (0 or 1 for no lesion or lesion\n",
        "at that voxel, respectively) and $X_i$ is the FLAIR value for voxel\n",
        "$i$. This solves the problem somewhat nicely, but it still leaves some\n",
        "issues unresolved. For example, what does probability even mean in\n",
        "this context? And also probabilities are between 0 and 1, that's\n",
        "better than exactly 0 or 1, but still would create problems.\n",
        "\n",
        "For the probability, it's generally a good idea to think about what\n",
        "you're modeling as random in the context. In this case, we're thinking\n",
        "of our voxels as a random sample of FLAIR and gold standard voxel\n",
        "values from some population. This is a meaningful benchmark even if\n",
        "it's not true. We'll find that often in statistics we model data as if\n",
        "it comes from a probability distribution when we know it didn't. We\n",
        "simply know that the probability distribution is a useful model for\n",
        "thinking about the problem.\n",
        "\n",
        "As for getting the probabilities from $[0,1]$ to $(-\\infty, \\infty)$,\n",
        "we need a function, preferably a monotonic one. The generally agreed\n",
        "upon choice is the logit (natural log of the odds) function. The logit\n",
        "function of a probability is defined as\n",
        "\n",
        "$$\n",
        "\\eta = \\mathrm{logit}(p) = \\log\\{p / (1 - p)\\}\n",
        "$$\n",
        "\n",
        "where $p$ is the probability and $O = p/(1-p)$ is called the\n",
        "**odds**. Note, you can go backwards from odds to probability with the\n",
        "function $p = O / (1 + O)$. Odds are exactly as used in gambling. If\n",
        "the odds of bet at 1 to 99, then you are saying the probability is $1\n",
        "/ (99 + 1) = 1\\%$.\n",
        "\n",
        "Why use odds? There's a couple of reasons why odds are uniquely\n",
        "interprettable. First, there are specific study designs where odds\n",
        "make more sense than probabilities, particularly retrospective\n",
        "ones. Secondly, odds are unique in binomial models where they work out\n",
        "to be particularly tractible to work with. Finally, odds have a unique\n",
        "gambling interpretation. That is, it gives the ratio of a one dollar\n",
        "risk to the return in a fair bet. (A fair bet is where the expected\n",
        "return is 0.) So, when a horse track gives the odds on a horse to be\n",
        "99 to 1, they are saying that you would get $99 dollars if you bet one\n",
        "dollar and the horse won. This is an implied probability of 99 / (99 +\n",
        "1) = 99% that the horse loses and 1% probability that the horse\n",
        "wins. Note they don't usually express it as a fraction, they usually\n",
        "espress it as value to 1 or 1 to value. So they would say 99 to 1\n",
        "(odds against) or 1 to 99 (odds for) so you can easily see how much\n",
        "you'd win for a dollar bet.\n",
        "\n",
        "You can go backwards from the logit function to the probability with\n",
        "the expit function. That is, if $\\eta$ is defined as above, then\n",
        "\n",
        "$$\n",
        "p = \\frac{e^{\\eta}}{1 + e^\\eta} = \\frac{1}{1 + e^{-\\eta}}.\n",
        "$$\n",
        "\n",
        "This is sometimes called the **expit** function or **sigmoid**.\n",
        "\n",
        "\n",
        "We model the log of the odds as linear. This is called **logistic regression**.\n",
        "\n",
        "$$\n",
        "\\eta = \\mathrm{logit}\\left\\{ P(Y = 1 ~|~ X) \\right\\}\n",
        "= \\beta_0 + \\beta_1 X.\n",
        "$$\n",
        "\n",
        "The nice part about this model is that $e^\\beta_1$ has the nice\n",
        "interpretation of the odds ratio associated with a one unit change in\n",
        "$X$.\n",
        "\n",
        "This is great, but we still need a function of the probabilities to\n",
        "optimize. We'll use the **cross entropy**.\n",
        "\n",
        "$$\n",
        "-\\sum_{i=1}^n \\left[Y_i \\log\\{P(Y_i = 1 ~|~ X_i)\\} + (1 - Y_i) \\log\\{1 - P(Y_i = 1 ~|~ X_i)\\}\\right].\n",
        "$$\n",
        "\n",
        "This function has the interpretation of being the negative of the log\n",
        "of the probabilities assuming the $Y_i$ are independent. This model\n",
        "doesn't have to hold for the minimization to be useful.\n",
        "\n",
        "Plugging our logit model in, the cross entropy now looks like\n",
        "\n",
        "$$\n",
        "-\\sum_{i=1}^n \\left[\n",
        "  Y_i \\eta_i + \\log\\left\\{\\frac{1}{1 + e^\\eta_i} \\right\\} \\right].\n",
        "$$\n",
        "\n",
        "This is the function that we minimize to perform logistic\n",
        "regression. Later on, we'll worry about how to minimize this\n",
        "function. However, today, let's fit logistic regression to some data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.linear_model as lm\n",
        "## this sets some style parameters\n",
        "sns.set()\n",
        "\n",
        "## Read in the data and display a few rows\n",
        "dat = pd.read_csv(\"https://raw.githubusercontent.com/bcaffo/ds4bme_intro/master/data/oasis.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## Plot the data\n",
        "sns.scatterplot(x = 'FLAIR', y = 'GOLD_Lesions', data = dat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now fit the model. Again we're going to split into training and\n",
        "test data. But, now we're not going to do it manually since we have to\n",
        "load a library that has a function to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x = dat[['FLAIR']]\n",
        "y = dat.GOLD_Lesions\n",
        "trainFraction = .75\n",
        "\n",
        "## Once again hold out some data\n",
        "sample = np.random.uniform(size = 100) < trainFraction\n",
        "xtrain = x[ sample]\n",
        "ytrain = y[ sample]\n",
        "xtest =  x[~sample]\n",
        "ytest =  y[~sample]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lr = lm.LogisticRegression(fit_intercept=True, penalty='none')\n",
        "fit = lr.fit(xtrain, ytrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at the parameters fit from the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "beta0, beta1 = [fit.intercept_[0], fit.coef_[0][0]]\n",
        "[beta0, beta1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "n = 1000\n",
        "xplot = np.linspace(-1, 5, n)\n",
        "eta = beta0 + beta1 * xplot\n",
        "p = 1 / (1 + np.exp(-eta))\n",
        "\n",
        "sns.scatterplot(x = 'FLAIR', y = 'GOLD_Lesions', data = dat, hue = 'GOLD_Lesions')\n",
        "sns.lineplot(x = xplot, y = p)\n",
        "\n",
        "## Of course, scikit has a predict\n",
        "## function so that you don't have to do this manually\n",
        "#yplot = fit.predict_proba(xplot.reshape((n, 1)))\n",
        "#sns.lineplot(xplot, yplot[:, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's evaluate the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## This predicts the classes using a 50% probability cutoff\n",
        "yhat_test = fit.predict(xtest)\n",
        "\n",
        "## double checking that if you want\n",
        "#all(yhat_test == (fit.predict_proba(xtest)[:, 1] > .5))\n",
        "\n",
        "accuracy = np.mean(yhat_test == ytest)\n",
        "sensitivity = np.mean(yhat_test[ytest == 1] == ytest[ytest == 1])\n",
        "specificity = np.mean(yhat_test[ytest == 0] == ytest[ytest == 0])\n",
        "np.round([accuracy, sensitivity, specificity], 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
        "\n",
        "ptest = fit.predict_proba(xtest)[:, 1]\n",
        "fpr, tpr, thresholds = roc_curve(ytest, ptest)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.figure()\n",
        "lw = 2 \n",
        "plt.plot(fpr, tpr, color='darkorange',\n",
        "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic example')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}